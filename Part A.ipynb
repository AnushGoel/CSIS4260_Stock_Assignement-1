{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e86230a3-272c-4f83-ab34-b286dac3bf55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "\n",
    "csv_file = \"all_stocks_5yr.csv\"\n",
    "\n",
    "data = pd.read_csv(csv_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b19404c9-08da-4c68-8cd3-ab66f2078077",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Scale  CSV Write Time (s)  CSV Read Time (s)  Parquet Write Time (s)  \\\n",
      "0    1x                2.22               0.39                    0.30   \n",
      "1   10x               21.43               2.82                    2.52   \n",
      "2  100x              225.40              38.81                   27.54   \n",
      "\n",
      "   Parquet Read Time (s)  CSV File Size (MB)  Parquet File Size (MB)  \n",
      "0                   0.11               28.80                   10.15  \n",
      "1                   0.63              288.01                   95.41  \n",
      "2                   7.02             2880.05                  951.91  \n"
     ]
    }
   ],
   "source": [
    "def scale_dataset(df, factor):\n",
    "    \"\"\"\n",
    "    Expands the dataset by duplicating its contents the specified number of times.\n",
    "    \"\"\"\n",
    "    return pd.concat([df.copy() for _ in range(factor)], ignore_index=True)\n",
    "\n",
    "# Scaling factors and corresponding labels\n",
    "scales = [1, 10, 100]\n",
    "scale_labels = [\"1x\", \"10x\", \"100x\"]\n",
    "benchmark_results = []\n",
    "\n",
    "# Conduct benchmark tests\n",
    "for factor, label in zip(scales, scale_labels):\n",
    "    expanded_data = data if factor == 1 else scale_dataset(data, factor)\n",
    "    \n",
    "    # Define file paths\n",
    "    csv_path = f\"expanded_data_{label}.csv\"\n",
    "    parquet_path = f\"expanded_data_{label}.parquet\"\n",
    "    \n",
    "    # Benchmark CSV write operation\n",
    "    start = time.time()\n",
    "    expanded_data.to_csv(csv_path, index=False)\n",
    "    csv_write_time = time.time() - start\n",
    "    \n",
    "    # Benchmark CSV read operation\n",
    "    start = time.time()\n",
    "    pd.read_csv(csv_path)\n",
    "    csv_read_time = time.time() - start\n",
    "    \n",
    "    # Benchmark Parquet write operation\n",
    "    start = time.time()\n",
    "    expanded_data.to_parquet(parquet_path, engine=\"pyarrow\", compression=\"snappy\", index=False)\n",
    "    parquet_write_time = time.time() - start\n",
    "    \n",
    "    # Benchmark Parquet read operation\n",
    "    start = time.time()\n",
    "    pd.read_parquet(parquet_path, engine=\"pyarrow\")\n",
    "    parquet_read_time = time.time() - start\n",
    "    \n",
    "    # Get file sizes in MB\n",
    "    csv_size = os.path.getsize(csv_path) / (1024 * 1024)\n",
    "    parquet_size = os.path.getsize(parquet_path) / (1024 * 1024)\n",
    "    \n",
    "    # Store benchmark results\n",
    "    benchmark_results.append({\n",
    "        \"Scale\": label,\n",
    "        \"CSV Write Time (s)\": round(csv_write_time, 2),\n",
    "        \"CSV Read Time (s)\": round(csv_read_time, 2),\n",
    "        \"Parquet Write Time (s)\": round(parquet_write_time, 2),\n",
    "        \"Parquet Read Time (s)\": round(parquet_read_time, 2),\n",
    "        \"CSV File Size (MB)\": round(csv_size, 2),\n",
    "        \"Parquet File Size (MB)\": round(parquet_size, 2)\n",
    "    })\n",
    "\n",
    "# Display results in DataFrame format\n",
    "benchmark_df = pd.DataFrame(benchmark_results)\n",
    "print(benchmark_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "689c030b-9b65-42e3-b5b7-187acdd9191f",
   "metadata": {},
   "source": [
    "# Data Format Performance Comparison\n",
    "\n",
    "This table compares the performance of CSV and Parquet formats based on various metrics (Read Time, Write Time, and File Size) at different scaling levels (1x, 10x, and 100x).\n",
    "\n",
    "| Scaling | CSV Write Time (s) | CSV Read Time (s) | Parquet Write Time (s) | Parquet Read Time (s) | CSV File Size (MB) | Parquet File Size (MB) |\n",
    "|---------|--------------------|-------------------|------------------------|----------------------|--------------------|-----------------------|\n",
    "| 1x      | 2.22               | 0.39              | 0.30                   | 0.11                 | 28.80              | 10.15                 |\n",
    "| 10x     | 21.43              | 2.82              | 2.52                   | 0.63                 | 288.01             | 95.41                 |\n",
    "| 100x    | 225.40             | 38.81             | 27.54                  | 7.02                 | 2880.05            | 951.91                |\n",
    "\n",
    "## Observations:\n",
    "\n",
    "### Write Time:\n",
    "- **CSV** write time increases significantly with scaling:\n",
    "  - From 2.22s at 1x to 225.40s at 100x.\n",
    "- **Parquet** write time also increases, but at a slower rate:\n",
    "  - From 0.30s at 1x to 27.54s at 100x.\n",
    "\n",
    "### Read Time:\n",
    "- **CSV** read time grows as well:\n",
    "  - From 0.39s at 1x to 38.81s at 100x.\n",
    "- **Parquet** read time increases slower:\n",
    "  - From 0.11s at 1x to 7.02s at 100x.\n",
    "\n",
    "### File Size:\n",
    "- **CSV** file sizes increase substantially:\n",
    "  - From 28.80MB at 1x to 2880.05MB at 100x.\n",
    "- **Parquet** file sizes also grow, but at a lower rate:\n",
    "  - From 10.15MB at 1x to 951.91MB at 100x.\n",
    "\n",
    "### Conclusion:\n",
    "- **Parquet** is more efficient than **CSV** in terms of both **file size** and **read time**, especially at larger scales.\n",
    "- However, **CSV** file sizes grow much faster, and **read/write times** become significantly slower as the dataset scales.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "476b0910-8a95-4bf2-bac4-202e575eeba3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
